###IMPORTS
import pandas as pd, sys, re
import argparse
import os,  time


###STATIC VARIABLES
folder_to_parse = '/mnt/home/groups/nmrl/bact_analysis/analysis_history/aquamis/'
pipeline_name = 'aquamis'
id_column_name = 'Sample_Name'
analysis_batch_map_path = '/mnt/home/groups/nmrl/bact_analysis/analysis_history/aquamis/aquamis_batch_map.csv'
output_folder_path = '/mnt/home/groups/nmrl/bact_analysis/bact_output/*/'
temp_target_file = "/mnt/home/groups/nmrl/bact_analysis/analysis_history/aquamis/temp_list.txt"
aquamis_found_tag = "aquamis*/confindr/*/confindr_log.txt"


###DERIVED VARIABLES
tag_string = f'{pipeline_name}_history_file'
log_path = f'{folder_to_parse}database_log_files/'
backup_path = f'{folder_to_parse}backup/'


###TIMESTAMP VARIABLES
cur_time = time.strftime("%d_%m_%Y_%H_%M_%S") #TO TIMESTAMP UPDATES OF SUMMARY FILES IN FILE NAME
cur_time_summary = time.strftime("%d_%m_%Y")
regex_cur_time_summary = r'[0-9]{2}_[0-9]{2}_[0-9]{4}'


###HELPER FUNCTIONS
def print_usage():
    '''Printing detailed help message to the terminal'''
    print('RUNNING WITH -p FLAG')
    print('INFO: The -b flag is used to integrate new batch report data into db file.')
    print('INFO: The script will automatically check if db file and batch report file contain matching sample ids.')
    print('INFO: The user will be prompted to confirm db file changing and backup file generation steps in the command line.')
    print('INFO: Duplicates.csv file is generated by this script if matching sample ids were found in new file and db file.')
    print('INFO: added_unique_records.csv file is generated by this script following corresponding user selection.\n')

    print('RUNNING WITH -d FLAG:')
    print('INFO: The -d flag is used to integrate duplicated values into db file.')
    print('INFO: Before running the script on the duplicates.csv file, the file should be manually edited by filling "processed" column as specified below.')
    print('INFO: values 0, 1, 2 should be entered into "processed" column of duplicates.csv to control the processing of duplicated data.')
    print('INFO: 0 - the record from db file should be replaced with record from batch report file.')
    print('INFO: 1 - the record from batch report file should be added to db file as new record (make sure to change the id, otherwise duplicated records will be created).')
    print('INFO: 2 - the record from batch report file should be dropped.')
    print('INFO: removed_duplicates.csv will be generated if value 2 was entered into "processed" column for any sample in duplicates.csv')


def parse_arguments() -> argparse.ArgumentParser:
    '''Parsing cmd arguments and return argparse class object.'''

    #CMD ARGUMENTS & SCRIPT USAGE MESSAGES
    parser = argparse.ArgumentParser(description='A script to update database file with new batch data.') 
    parser.add_argument('-d', '--dupl', metavar='\b', help = 'Full path to the duplicates csv files', default=None, required=False)
    parser.add_argument('-p', '--pipe', metavar='\b', help = f'Full path to the {pipeline_name} report', default=None, required=False)


    #IF SCRIPT IS RUN WITHOUT ARGUMENTS - PRINT HELP MESSAGE & EXIT
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        print_usage()
        sys.exit(1)
    return parser.parse_args()


def prepare_pipe_data(ardetype_report_path:str) -> pd.DataFrame:
    '''
    Given path to ardetype report, restructures it by concatenating hmethod, 
    type and reference columns for each sample into single column, sparated by semicolon (;).
    If all fields are empty for any given sample, empty columns are produced for that sample. 
    Returns reformatted pandas dataframe.
    '''
    df = pd.read_csv(ardetype_report_path).applymap(str)
    tools = [col.replace("method|", "") for col in df.columns if "method" in col]
    print(tools)
    if tools: #IF SPECIES-SPECIFIC TYPING WAS PERFORMED
        df["methods"] = df[[col for col in df.columns if "method|" in col]].agg(' ; '.join, axis=1) #CONCATENATE METHODS
        df["types"] = df[[col for col in df.columns if "type|" in col]].agg(' ; '.join, axis=1) #CONCATENATE TYPES
        df["types"] = [(' ; ').join([tools[i]+": "+value for i,value in enumerate(row.split(' ; '))]) for row in df['types']] #ADD TOOL-BASED ANNOTATION TO EACH TYPE
        df["references"] = df[[col for col in df.columns if "reference|" in col]].agg(' ; '.join, axis=1) #CONCATENATE REFERENCES
        df.drop(list(df.filter(regex = '(method\||type\||reference\|)')), axis = 1, inplace = True) #REMOVE UNUSED COLUMNS
        df = df.replace('((; |)nan( |)|; [A-z0-9]*: nan |[A-z0-9]*: nan ; |[\-A-z0-9]*: nan)','', regex=True) #REMOVE REDUNDANT INFORMATION
        df = df.replace('(; $|^; )','', regex=True) #REMOVE REDUNDANT SEPARATORS
        df.drop('taxid', inplace=True, axis=1) #DROP UNUSED COLUMN
    else:
        #BLANK ENTRIES FOR SAMPLES WHERE NO SPECIFIC DATA IS AVAILABLE
        df['methods'] = ["" for _ in df.index]
        df['types'] = ["" for _ in df.index] 
        df['references'] = ["" for _ in df.index]
        df.fillna("", inplace=True)
    return df


def find_history_file(folder_to_parse:str, tag_string:str) -> str:
    '''Parses the specified folder to find file path that contains tag string. Returns the string or None if not found.'''
    db_file_name = [file for file in os.listdir(folder_to_parse) if tag_string in file][0] #LOOKUP FOR THE DATABASE FILE IN THE FOLDER WHERE SCRIPT IS LOCATED
    if folder_to_parse[-1] == "/":
        db_file_path = folder_to_parse + db_file_name
    else:
        db_file_path = f'{folder_to_parse}/{db_file_name}'
    return db_file_path


def parse_pipe_files(pipe_report_path:str, db_file_path:str, pipe_sep:str='\t') -> tuple([pd.DataFrame, pd.DataFrame]):
    '''Reads pipe_report_path and db_file_path into pandas dataframes. Returns tuple of two dataframes.'''
    ###READ DATA FROM FILES

    pipe_data = pd.read_csv(pipe_report_path, sep=pipe_sep) #READ PIPELINE REPORT
    db_frame = pd.read_csv(db_file_path).applymap(str) #NORMALIZE THE DB FILE DATAFRAME TO STRING TYPE
    columns_to_exclude = ['analysis_batch_id'] #TO STORE METADATA COLUMNS NAMES
    db_frame = db_frame.loc[:, ~db_frame.columns.isin(columns_to_exclude)] #LEAVE OUT METADATA COLUMNS
    ###PRE-PROCESS PIPELINE REPORT DATA
    batch_data = pipe_data[db_frame.columns] #EXTRACTING & REORDERING RELEVANT COLUMNS
    batch_data.fillna(0, inplace=True) #STANDARDIZE EMPTY RECORDS
    return batch_data, db_frame
    

def parse_dupl_files(duplicate_file_path:str, db_file_path:str) -> tuple([pd.DataFrame, pd.DataFrame]):
    '''Reads duplicate_file_path and db_file_path into pandas dataframes. Returns tuple of two dataframes.'''
    duplicate_df = pd.read_csv(duplicate_file_path) #READ PIPELINE REPORT
    columns_to_exclude = ['analysis_batch_id'] #TO STORE METADATA COLUMNS NAMES
    db_frame = pd.read_csv(db_file_path).applymap(str) #NORMALIZE THE DB FILE DATAFRAME TO STRING TYPE
    db_frame = db_frame.loc[:, ~db_frame.columns.isin(columns_to_exclude)] #LEAVE OUT METADATA COLUMNS
    db_frame = pd.read_csv(db_file_path).applymap(str) #NORMALIZE THE DB FILE DATAFRAME TO STRING TYPE
    return duplicate_df, db_frame


def match_duplicates(batch_data:pd.DataFrame, db_frame:pd.DataFrame, id_cln_name:str) -> tuple([pd.DataFrame, pd.DataFrame, pd.DataFrame]):
    '''
    Matches rows in pipe_report frame against db_file frame.
    Returns dataframe of booleans indicating if each value from pipe_report frame is present in db_file_frame.
    '''
    batch_data.drop_duplicates(subset=id_cln_name, keep='first', inplace=True) #REMOVE RECORDS WHERE SAMPLE ID IS DUPLICATED

    ###REINDEXING
    db_frame[f'{id_cln_name}_1'] = db_frame[id_cln_name] #REPEAT SAMPLE ID COLUMN TO USE IN INDEXING IN DB FILE DATAFRAME
    batch_data[f'{id_cln_name}_1'] = batch_data[id_cln_name] #REPEAT SAMPLE ID COLUMN TO USE IN INDEXING IN BATCH DATAFRAME
    reind_df_db = db_frame.set_index(f'{id_cln_name}_1') #CREATE A COPY OF DB FILE DATAFRAME USING SAMPLE IDS AS INDEXES
    reind_df_db.index = reind_df_db.index.astype('str')
    reind_df_batch = batch_data.set_index(f'{id_cln_name}_1') #CREATE A COPY OF BATCH DATAFRAME USING SAMPLE IDS AS INDEXES IN COMPARISON BETWEEN DB FILE AND BATCH FILE
    reind_df_batch.index = reind_df_batch.index.astype('str')
    reind_df_batch[id_cln_name]= reind_df_batch[id_cln_name].astype('str')
    
    ###MATCHING PIPELINE AGAINST DATABASE DATAFRAMES
    match = reind_df_batch.isin(reind_df_db) #IF ALL ROW-COLUMN COMBINATIONS FROM BATCH DATAFRAME WITH ROW-COLUMN COMBINATIONS FROM DB FILE DATAFRAME
    match.index = match.index.astype(str) #NORMALIZE INDEXES AS TYPE STRING
    return match, batch_data, db_frame


def separate_duplicates(
    match_df:pd.DataFrame, 
    batch_df:pd.DataFrame,
    id_cln_name:str) -> tuple([pd.DataFrame, pd.DataFrame, pd.DataFrame]):
    '''Separating duplicated records from unique records, returning three separate dataframes containing match booleans, duplicated records and unique records.'''

    samples = match_df.index #GET ALL SAMPLE IDS IN THE MATCH MATRIX
    condition_dup_smpl = match_df[id_cln_name] == True #CONDITION TO FIND SAMPLE IDS THAT ARE FOUND BOTH IN DB FILE AND BATCH FILE
    
    condition_unq_smpl = match_df[id_cln_name] == False #CONDITION TO FIND SAMPLE IDS THAT ARE FOUND ONLY IN BATCH FILE
    unique_samples = list(map(str, list(samples[condition_unq_smpl]))) #EXTRACTING SAMPLE IDS OF SAMPLES UNIQUE TO BATCH FILE
    duplicate_samples = list(map(str, list(samples[condition_dup_smpl]))) #EXTRACTING IDS OF SAMPLES THAT ARE FOUND BOTH IN DB FILE AND BATCH FILE
    
    indexes = batch_df.index #GET INDEXES OF BATCH FILE
    # condition_unq_idx = batch_df[id_cln_name].isin(unique_samples) #CONDITION TO EXTRACT BATCH FILE INDEXES OF SAMPLES UNIQUE TO BATCH FILE
    condition_dup_idx = batch_df[id_cln_name].isin(duplicate_samples) #CONDITION TO EXTRACT BATCH FILE INDEXES OF SAMPLES THAT ARE FOUND BOTH IN DB FILE AND BATCH FILE
    
    #unique_indexes = list(indexes[condition_unq_idx]) #EXTRACTING BATCH FILE INDEXES OF SAMPLES UNIQUE TO BATCH FILE
    duplicate_indexes = list(indexes[condition_dup_idx]) #EXTRACTING BATCH FILE INDEXES OF SAMPLES THAT ARE FOUND BOTH IN DB FILE AND BATCH FILE
    duplicate_frame = batch_df.iloc[duplicate_indexes] #GENERATING FRAME WITH DATA FOR SAMPLES THAT ARE FOUND BOTH IN DB FILE AND BATCH FILE
    
    unique_records = batch_df.drop(duplicate_indexes) #KEEPING ONLY DATA ON SAMPLES UNIQUE TO BATCH FILE IN THE BATCH FILE FRAME
    unique_records.drop([f'{id_cln_name}_1'], axis=1, inplace=True) #REMOVING COLUMN USED FOR INDEXING
    match = match_df.drop(unique_samples) #REMOVING UNIQUE ENTRIES FROM MATCH MATRIX AND SAVING IT TO BE USED IN DUPLICATE PROCESSING
    return match, duplicate_frame, unique_records


def add_unique(
    unique_rec_df:pd.DataFrame, 
    db_file_df:pd.DataFrame, 
    db_file_path:str, 
    log_folder_path:str, 
    backup_folder_path:str, 
    id_cln_name:str, 
    log_timestamp:str, 
    summary_timestamp:str,
    summary_timestamp_regex:str
    ):
    '''
    Provides command-line interface to add unique records from pipeline report to the database file.
    Creates a backup of database file and logs added records in designated folders.
    '''
    
    update_unique_to_df = input('Would you like to add unique entries into DB file? (Y\y)/(N/n): ')
    if update_unique_to_df in ['y','Y']:
        print('Unique records were appended to DB file.')
        record_unique = input('Would you like to save added entries in separate csv file? (Y\y)/(N/n): ')
        
        if record_unique in ['y','Y']:
            print('Saving added unique records into separate csv file.')
            os.system(f'mv {db_file_path} {backup_folder_path}') #BACKING-UP DATABASE FILE TO REVERT UPDATE IF NEEDED
            unique_rec_df.to_csv(f'{log_folder_path}added_unique_records_{log_timestamp}.csv', header=True, index=False)
        
        elif record_unique in ['n','N']:
            print('Added records:\n')
            print(unique_rec_df)
        
        else: 
            print('Invalid user input on unique records save.')
            sys.exit(1)
        if not unique_rec_df.empty: db_file_df = pd.concat([db_file_df, unique_records], sort=False) #ADDING UNIQUE RECORDS TO DB FILE FRAME
        db_file_df.drop([f'{id_cln_name}_1'], axis=1, inplace=True) #REMOVING COLUMN USED IN INDEX SEARCH
        db_file_new = db_file_path.replace(re.search(summary_timestamp_regex,db_file_path).group(0),summary_timestamp)
        db_file_df.to_csv(db_file_new,header=True, index=False) #UPDATING DB FILE

    elif update_unique_to_df in ['n','N']:
        print(unique_rec_df)
        print('Unique records were NOT added to DB file.')
    
    else: 
        print('Invalid user input on db file update.')
        sys.exit(1)


def check_dupl_mismatch(match_df:pd.DataFrame, db_df:pd.DataFrame, duplicate_df:pd.DataFrame, id_cln_name:str, db_home_path:str, duplicate_exp_timestamp:str):
    '''Provides command-line interface to search duplicated records from pipeline report against a database file.
    If there are any mismatches in rows contained in database file and the pipeline report, indicates it in the command line.
    Provides the option to export the duplicated records as separate file, which can then be edited manually and passed to the script
    edit corresponding records in the database file based on entered information.'''

    duplicate_df.drop([f'{id_cln_name}_1'], axis=1, inplace=True) #REMOVE REDUNDANT INDEXING COLUMN FROM DUPLICATE FRAME
    duplicate_df.reset_index(drop=True, inplace=True) #RESET INDEXES FOR DUPLICATE FRAME
    match_df.reset_index(drop=True, inplace=True) #RESET INDEXES FOR MATCH MATRIX (REQUIRED TO MATCH INDEXES OF DUPLICATE FRAME AND MATCH MATRIX AFTER UNIQUE ENTRIES WERE REMOVED FROM MATCH MATRIX)
    
    dup_indexes = [idx for idx in match_df.index if all(match_df.iloc[idx])] #GENERATE LIST OF INDEXES FOR ROWS THAT FULLY MATCH ROWS FROM DB FILE (IDENTICAL VALUES IN ALL COLUMNS)
    duplicate_df = duplicate_df.drop(dup_indexes) #REMOVING FULL MATCHES BY INDEX
    duplicate_list = list(duplicate_df[id_cln_name]) #GENERATING LIST OF SAMPLE IDS THAT REMAIN AFTER FULL MATHCES WERE EXCLUDED
    db_duplicates = db_df[db_df[id_cln_name].isin(duplicate_list)] #GENERATING NEW FRAME FROM DB FILE CONTAINING ONLY SAMPLES WITH NOT-FULLY MATCHED ROWS (VALUES IS SOME COLUMNS DIFFER, BUT IDS ARE THE SAME)
    db_duplicates = db_duplicates.add_suffix('_DB') #ADDING SUFFIX TO COLUMNS EXTRACTED FROM DB FILE
    db_duplicates = db_duplicates.rename(columns={f'{id_cln_name}_DB':id_cln_name}) #NORMALISING COLUMN NAME TO MERGE UPON
    merge_product = duplicate_df.merge(db_duplicates, on=id_cln_name, how='outer') #COMBINE VALUES EXTRACTED FROM DB FILE WITH VALUES EXTRACTED FROM BATCH FILE (KEEP BOTH VALUES)


    if not merge_product.empty: #IF THERE ARE ANY PARTIAL MATCHES BETWEEN DB FILE AND BATCH FILE FOR THE SAME ID - PROCESS DUPLICATE FRAME
        print('\nThe following duplicate rows are found to contain mismatches for the same id in the db file.')
        print(merge_product)
        merge_product = merge_product[sorted(merge_product.columns, reverse=True)] #SORT COLUMNS SO THAT _DB-SUFFIXED COLUMNS ARE NEAR COLUMNS FROM BATCH FILE
        merge_product['process'] = [None for _ in merge_product.index] #ADD PROCESS COLUMN TO BE USED IN FURTHER PROCESSING OF DUPLICATE VALUES
        duplicates_to_csv = input('Would you like to save list of duplicates as csv? (Y\y)/(N/n): ')
        
        if duplicates_to_csv in ['y','Y']:
            print('Saving duplicates to csv.')
            merge_product.to_csv(f'{db_home_path}/duplicates_{duplicate_exp_timestamp}.csv', header=True, index=False) #SAVE DUPLICATE FRAME TO CSV IF PROMPTED BY THE USER
        
        elif duplicates_to_csv in ['n','N']:
            print('Duplicate record export skipped.')
        
        else: 
            print('Invalid user input on duplicate records save.')
            sys.exit(1)


def validate_duplicate_file(duplicate_frame:pd.DataFrame):
    '''Stops the script if the duplicate frame does not contain "process" column'''
    if 'process' not in duplicate_frame.columns: #IF THERE IS NO PROCESS COLUMN IN DUPLICATES FILE, ABORT - COLUMN IS REQUIRED TO PROCESS DUPLICATE VALUES
        sys.exit('Error: No process column in provided file.')


def discard_duplicates(duplicate_frame:pd.DataFrame, log_timestamp:str, log_folder_path:str) -> pd.DataFrame:
    '''Removes duplicates that must be discarded and returns duplicate dataframe containing the remaining records.'''
    columns = [col for col in duplicate_frame.columns if 'DB' not in col] #GET LIST OF COLUMNS FROM BATCH FILE
    duplicate_frame=duplicate_frame[columns] #KEEP ONLY COLUMNS FROM BATCH FILE IN DUPLICATES FRAME

    if 2 in duplicate_frame['process']: #VALUE IN 'PROCESS' COLUMN == 2 - DATA IS TO BE DISCARDED
        print('Exporting list of dropped duplicates.')
        removed_duplicates = duplicate_frame[duplicate_frame['process'] == 2]
        if len(removed_duplicates) > 0:
            removed_duplicates.to_csv(f'{log_folder_path}discarded_duplicates_{log_timestamp}.csv',header=True, index=False) #GENERATE A CSV CONTAINING DISCARDED DATA FOR BACKUP PURPOSES
        duplicate_frame=duplicate_frame[duplicate_frame['process'] != 2] #DROPPING ROWS BASED ON 'PROCESS' COLUMN VALUE
    return duplicate_frame


def replace_duplicates(duplicate_frame:pd.DataFrame, db_df:pd.DataFrame, id_cln_name:str) -> pd.DataFrame:
    '''Replaces records in the db_frame using data from duplicate_frame. Returns updated db_frame.'''

    replace = duplicate_frame[duplicate_frame['process'] == 0] #VALUE IN 'PROCESS' COLUMN == 0 - DATA IN DB FILE IS TO BE REPLACED WITH DATA FROM DUPLICATES FILE
    replace = replace[[col for col in replace.columns if col != 'process']] #REMOVING 'PROCESS' COLUMN FROM REPLACE FRAME
    print('The following data will replace data from DB.')
    print(replace)
    if not replace.empty:
        replace[id_cln_name] = replace[id_cln_name].astype(str) #NORMALISING SAMPLE IDS AS TYPE STRING IN REPLACE FRAME
        id_list = list(replace[id_cln_name]) #GETTING SAMPLE IDS FROM REPLACE FRAME
        index = db_df.index #GETTING INDEXES FROM DB FILE FRAME
        condition = db_df[id_cln_name].isin(id_list) #CONDITION TO EXTRACT INDEXES FROM DB FILE FRAME
        rep_indexes = index[condition].to_list() #GETTING LIST OF INDEXES WHERE REPLACEMENT WAS NEEDED
        print(rep_indexes)
        db_df.drop(rep_indexes, inplace=True) #REMOVING ROWS THAT REQUIRED REPLACEMENT BY INDEXES
        db_df = pd.concat([db_df, replace], sort=False) #APPENDING ROWS FROM REPLACE FRAME TO DB FILE FRAME
    return db_df


def add_duplicates(duplicate_frame:pd.DataFrame, db_df:pd.DataFrame) -> pd.DataFrame:
    '''Adds records to the db_frame using data from duplicate_frame. Returns updated db_frame.'''
    append = duplicate_frame[duplicate_frame['process'] == 1] #VALUE IN 'PROCESS' COLUMN == 1 - DATA FROM DUPLICATES FILE IS TO BE ADDED AS NEW DATA TO THE DB FILE (SAMPLE ID CHANGING OPTION)
    append = append[[col for col in append.columns if col != 'process']] #REMOVING 'PROCESS' COLUMN FROM APPEND FRAME
    print('The following data will be added to the database.')
    print(append)
    if not append.empty:
        db_df = pd.concat([db_df, append], sort=False) #APPENDING VALUES FROM APPEND FRAME TO DB FILE FRAME
    return db_df


def update_analysis_batch(batch_map_path:str, db_file_path:str, parent_dir_path:str, id_cln_name:str, temp_target_path:str, found_tag:str, ):
    '''Extracts metadata from tree structutre of the output folder and adding it to the db file. Search results are stored in batch_map files between the runs.'''
    batch_map = pd.read_csv(batch_map_path) #READ DATABASE FILE
    db_df = pd.read_csv(db_file_path) #READ BATCH FILE
    search_list = db_df[id_cln_name][~db_df[id_cln_name].isin(batch_map[id_cln_name])] #GET LIST OF FILES IN DB THAT ARE NOT IN BATCH MAP
    os.system(f'echo path > {temp_target_path} ; for i in {parent_dir_path}/{found_tag}; do echo $i; done >> {temp_target_path}') #EXPORT ALL PATHS TO SAMPLE-LABELED FOLDERS TO A FILE
    path_df = pd.read_csv(temp_target_path)['path'].str.split('/', expand=True) #READ FILE TO TABLE AND SPLIT BY SEPARATORS "/" INTO COLUMNS
    os.system(f'rm {temp_target_path}') #REMOVE TEMP FILE
    if search_list.empty:
        filtered_df = path_df[[7,10]]
    else:
        filtered_df = path_df[path_df[10].isin(search_list)][[7,10]] #KEEP INFORMATION ABOUT THE SAMPLES THAT MUST BE SEARCHED
    filtered_df = path_df[path_df[10].isin(search_list)][[7,10]] #KEEP INFORMATION ABOUT THE SAMPLES THAT MUST BE SEARCHED
    filtered_df.columns = ['analysis_batch_id', id_cln_name] #RENAMING DF COLUMNS
    batch_map = pd.concat([batch_map, filtered_df], sort=False) #COMBINE BATCH MAP WITH 
    batch_map.drop_duplicates(subset=[id_cln_name], inplace=True, keep='first') #ONLY ONE BATCH ID PER SAMPLE EXPECTED
    batch_map.to_csv(batch_map_path, header=True, index=False) #SAVE SCANNING RESULTS FOR SPEED-UP
    db_df.drop('analysis_batch_id', axis=1, inplace=True)
    db_df = pd.merge(db_df, batch_map, on=id_cln_name) #ADD ANNOTATION COLUMN
    db_df.dropna(axis=0, how='all', thresh=None, subset=db_df.columns[1:-1], inplace=True)
    db_df.to_csv(db_file_path, header=True, index=False) #SAVE UPDATED DB FILE


if __name__ == "__main__":
    db_file_path = find_history_file(folder_to_parse, tag_string) #LOOKUP FOR THE DATABASE FILE IN THE FOLDER WHERE SCRIPT IS LOCATED
    print(f'Current database file: {db_file_path}') #REPORTING CURRENT DB FILE
    args = parse_arguments() #CMD ARGUMENTS & SCRIPT USAGE MESSAGES

    if args.pipe: #IF SCRIPT IS USED TO ADD DATA FOR A NEW BATCH TO DATABASE FILE
        data_tuple = parse_pipe_files(args.pipe, db_file_path) #READING PIPELINE REPORT AND DATABASE FILES
        matching_tuple = match_duplicates(data_tuple[0], data_tuple[1], id_column_name) #CHECKING IF PIPELINE REPORT CONTAINS RECORDS THAT ARE ALREADY IN DB FILE 

        duplicates = any(matching_tuple[0][id_column_name]) #CHECKING IF THERE IS A MATCH IN SAMPLE ID BETWEEN DB FILE AND BATCH FILE
        if duplicates: #IF MATCH IS FOUND
            duplicate_tuple = separate_duplicates(matching_tuple[0], matching_tuple[1], id_column_name) #SEPARETE DUPLICATED AND UNIQUE RECORDS
            add_unique(duplicate_tuple[2], matching_tuple[2], db_file_path, log_path, backup_path, id_column_name, cur_time, cur_time_summary, regex_cur_time_summary) #ADD UNIQUE TO THE DB FILE
            check_dupl_mismatch(duplicate_tuple[0], matching_tuple[2], duplicate_tuple[1], id_column_name, folder_to_parse, cur_time_summary) #SCAN DB FOR PARTIAL MISMATCHES IN DUPLICATES
        else: #IF THERE IS NO MATCH IN SAMPLE ID BETWEEN DB FILE AND BATCH FILE
            unique_records = matching_tuple[1] #CONSIDER ALL ENTRIES IN BATCH FILE UNIQUE
            add_unique(unique_records, matching_tuple[2], db_file_path, log_path, backup_path, id_column_name, cur_time, cur_time_summary, regex_cur_time_summary) #ADD UNIQUE TO THE DB FILE


    if args.dupl: #IF DUPLICATE PROCESSING FLAG WAS SET
        #PROCESSING DUPLICATE FILE
        data_tuple = parse_dupl_files(args.dupl, db_file_path)
        validate_duplicate_file(data_tuple[0])
        remaining_duplicates = discard_duplicates(data_tuple[0], cur_time, log_path)
        db_after_replace = replace_duplicates(remaining_duplicates, data_tuple[1], id_column_name)
        db_after_add = add_duplicates(remaining_duplicates, db_after_replace)

        #UPDATING DB FILE
        db_file_new = db_file_path.replace(re.search(regex_cur_time_summary,db_file_path).group(0),cur_time_summary)
        db_after_add.to_csv(db_file_new,header=True, index=False) 
        os.system(f'mv {args.dupl} {log_path}')


    ###CLEANUP
    update_analysis_batch(
        analysis_batch_map_path,
        db_file_path,
        output_folder_path,
        id_column_name,
        temp_target_file,
        aquamis_found_tag
    )
    os.system(f'chmod -R 775 {folder_to_parse}')
