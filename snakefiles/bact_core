localrules: all, contig_id

import sys, pandas as pd
sys.path.insert(0, os.path.abspath('./'))
from subscripts.ardetype_utilities import Ardetype_housekeeper as hk


sip_wild     = config['work_dir']+'{sample_id_pattern}_R[1,2]_001.fastq.gz'
sample_sheet = pd.read_csv(f"{config['output_directory']}sample_sheet.csv")

def choose_assembly_branch(wildcards):
    import os
    ont_present = os.path.exists(config['output_directory']+f'{wildcards.sample_id_pattern}_ONT.fastq.gz')
    ill_present = os.path.exists(config['output_directory']+f'{wildcards.sample_id_pattern}_R1_001.fastq.gz') or os.path.exists(config['work_dir']+f'{wildcards.sample_id_pattern}_R1_001.fastq.gz')
    condition_hybr = ont_present and ill_present
    condition_ill = ill_present and not ont_present
    condition_ont = ont_present and not ill_present

    if condition_hybr:
        return config['output_directory']+f'{wildcards.sample_id_pattern}_poly_polished.fasta.PolcaCorrected.fa'
    elif condition_ill:
        return f'{config["output_directory"]}'+f'{wildcards.sample_id_pattern}_contigs/contigs.fa'
    elif condition_ont:
        return config['output_directory']+f'{wildcards.sample_id_pattern}.circularized.fasta'



rule all:
    input: 
        config["core_target_files"],                                                                            #reads all expected outputs from config file                                                                                   
        k2c_list = expand(
            config['output_directory']+'{sample_id}_kraken2_contigs_report.txt', 
            sample_id=sample_sheet['sample_id']),                                                              #Creates a list of kraken contig reports to summarize taxonomy for all samples
        k2r_list = expand(
            config['output_directory']+'{sample_id}_kraken2_reads_report.txt', 
            sample_id=sample_sheet['sample_id']),

        # list of all contig files in output
        contig_list = expand(
            config['output_directory']+'{sample_id}_contigs.fasta', 
            sample_id=sample_sheet['sample_id'])
    
    run:
        total_dict = {}
        for file in input.k2c_list:                                                                             #looping over reports
            df         = pd.read_table(file, header=None)[[0,3,5]]                                              #read columns
            df.columns = ['read_%','taxid','name']                                                              #rename 
            df         = df[df['taxid'] == "S"].reset_index(drop=True)                                          #extract only species rows
            top_hit    = df.loc[df["read_%"] ==df["read_%"].max()]['name'].reset_index(drop=True)[0].strip()    #extract top hit
            total_dict[
                os.path.basename(file).replace("_kraken2_contigs_report.txt","")                              
                ]      = top_hit                                                                                #save info for current samples
        with open(config['output_directory']+'core_aggregated_taxonomy.json', "w+") as json_handle:             #write top hit for each sample to json file
            json.dump(total_dict, json_handle, indent=4)
        

        #Aggregate kraken2 read results
        kraken2reads_summary = hk.aggregator(outfolder_path = config['output_directory'], proc_num = 6, extractor = hk.kraken2reads_results, pathlist = input.k2r_list)
        kraken2reads_summary.to_csv(f"{config['output_directory']}kraken2reads_report.csv", header=True, index=False)

        #Aggregate kraken2 contig results
        kraken2contigs_summary = hk.aggregator(outfolder_path = config['output_directory'], proc_num = 6, extractor = hk.kraken2contigs_results, pathlist = input.k2c_list)
        kraken2contigs_summary.to_csv(f"{config['output_directory']}kraken2contigs_report.csv", header=True, index=False)
        
        print('bact_core finished')

rule illumina_qc:
    input:
        sif_file = ancient(config["fastp_sif"]),                                                                         #path to singularity image file
        read_1   = ancient(config['work_dir']+'{sample_id_pattern}_R1_001.fastq.gz'),
        read_2   = ancient(config['work_dir']+'{sample_id_pattern}_R2_001.fastq.gz')
    threads: 4                                                                                                  #to be moved to config
    envmodules:
        'singularity'
    output: 
        config['output_directory']+'{sample_id_pattern}.fastp.json',
        config['output_directory']+'{sample_id_pattern}.fastp.html',
        read_1_tr = config['output_directory']+'{sample_id_pattern}_fastp_1.fastq.gz',
        read_2_tr = config['output_directory']+'{sample_id_pattern}_fastp_2.fastq.gz'
    shell:
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.sif_file} fastp -j \
        {config[output_directory]}{wildcards.sample_id_pattern}.fastp.json \
        -h {config[output_directory]}{wildcards.sample_id_pattern}.fastp.html \
        --in1 {input.read_1} --in2 {input.read_2} \
        --out1 {output.read_1_tr} --out2 {output.read_2_tr} \
        --thread {threads} \
        --max_len1 {config[core_tool_configs][fastp][max_len1]} \
        --max_len2 {config[core_tool_configs][fastp][max_len2]}
        """


rule ont_qc:
    input:
        sif_file = config["snikt_sif"],
        fastq = ancient(config['output_directory']+'{sample_id_pattern}_ONT.fastq.gz')
    output:
        config['output_directory']+'{sample_id_pattern}_snikt.html'
    params:
        s=config["core_tool_configs"]["snikt"]["sample_size"]
    envmodules:
        'singularity'
    shell:
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} \
        {input.sif_file} snikt.R -s {params.s} --notrim {input.fastq} -o {wildcards.sample_id_pattern}_snikt -w {config[output_directory]}
        """


rule ont_filtering:
    input:
        sif_file = config["filtlong_sif"],
        fastq = config['output_directory']+'{sample_id_pattern}_ONT.fastq.gz'
    output:
        config['output_directory']+'{sample_id_pattern}_filtered.fastq.gz'
    params:
        keep_percent=config["core_tool_configs"]["filtlong"]["keep_percent"]
    envmodules:
        'singularity'
    shell:
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} \
        {input.sif_file} filtlong --verbose --keep_percent {params.keep_percent} {input.fastq} | gzip > {output} \
        """


rule ont_assembly:
    input:
        sif_file = config["flye_sif"],
        fastq = config['output_directory']+'{sample_id_pattern}_filtered.fastq.gz'
    output:
        config['output_directory']+'{sample_id_pattern}.flyed.fasta'
    params:
        threads=config["core_tool_configs"]["flye"]["threads"]
    envmodules:
        'singularity'
    shell:
        """
        folder_path={config[output_directory]}/{wildcards.sample_id_pattern}_flye
        if [ -d $folder_path ]; then
            rm -r {config[output_directory]}/{wildcards.sample_id_pattern}_flye
        fi
        mkdir -p {config[output_directory]}/{wildcards.sample_id_pattern}_flye
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.sif_file} flye --nano-raw {input.fastq} --out-dir {config[output_directory]}/{wildcards.sample_id_pattern}_flye --threads {params.threads}
        cp {config[output_directory]}/{wildcards.sample_id_pattern}_flye/assembly.fasta {output}
        """

rule circlator:
    input:
        sif_file = config["circlator_sif"],
        assembly = config['output_directory']+'{sample_id_pattern}.flyed.fasta',
    output:
        temp(config['output_directory']+'{sample_id_pattern}_circlator.prodigal.for_prodigal.fa'),
        temp(config['output_directory']+'{sample_id_pattern}_circlator.prodigal.prodigal.gff'),
        temp(config['output_directory']+'{sample_id_pattern}_circlator.promer.contigs_with_ends.fa'),
        temp(config['output_directory']+'{sample_id_pattern}_circlator.promer.promer'),
        temp(config['output_directory']+'{sample_id_pattern}_circlator.detailed.log'),
        contigs = temp(config['output_directory']+'{sample_id_pattern}_circlator.fasta'),
        assembly = config['output_directory']+'{sample_id_pattern}.circularized.fasta',
        logs = config['output_directory']+'{sample_id_pattern}_circlator.log'
    envmodules:
        'singularity'
    shell:
        """
        folder_path={config[output_directory]}/{wildcards.sample_id_pattern}_circlator
        if [ -d $folder_path ]; then
            rm -r {config[output_directory]}/{wildcards.sample_id_pattern}_circlator
        fi
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.sif_file} circlator fixstart {input.assembly} {config[output_directory]}/{wildcards.sample_id_pattern}_circlator
        cp {output.contigs} {output.assembly}
        """


rule medaka:
    input:
        medaka_image = config["medaka_sif"],
        ont_reads = config['output_directory']+'{sample_id_pattern}_filtered.fastq.gz',
        fixed_start_assembly = config['output_directory']+'{sample_id_pattern}.circularized.fasta',
    output:
        medaka_cor_consensus = config['output_directory']+'{sample_id_pattern}.medaka.fasta'
    params:
        threads=config["core_tool_configs"]["medaka"]["threads"]
    envmodules:
        'singularity'
    shell:
        """
        MEDAKA_TEMP={config[output_directory]}{wildcards.sample_id_pattern}_medaka_tmp/
        if [ -d ${{MEDAKA_TEMP}} ]; then
            rm -r ${{MEDAKA_TEMP}}
        fi
        mkdir -m 775 ${{MEDAKA_TEMP}}
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.medaka_image} medaka_consensus -i {input.ont_reads} -d {input.fixed_start_assembly} -o ${{MEDAKA_TEMP}} -t {params.threads}
        mv ${{MEDAKA_TEMP}}consensus.fasta {output.medaka_cor_consensus}
        rm -r ${{MEDAKA_TEMP}}
        """


rule ont_polishing:
    input:
        sif_file = config["polypolish_sif"],
        bwa_sif = config["bwa_sif"],
        assembly = config['output_directory']+'{sample_id_pattern}.medaka.fasta',
        read_1 = config['output_directory']+'{sample_id_pattern}_fastp_1.fastq.gz',
        read_2 = config['output_directory']+'{sample_id_pattern}_fastp_2.fastq.gz'
    output:
        alignments_1 = temp(config['output_directory']+'{sample_id_pattern}_alignments_1.sam'),
        alignments_2 = temp(config['output_directory']+'{sample_id_pattern}_alignments_2.sam'),
        filt_align_1 = temp(config['output_directory']+'{sample_id_pattern}_filtered_1.sam'),
        filt_align_2 = temp(config['output_directory']+'{sample_id_pattern}_filtered_2.sam'),
        poly_polished_fasta = config['output_directory']+'{sample_id_pattern}_poly_polished.fasta',
    params:
        low_filt_th=config["core_tool_configs"]["polypolish"]["min_th"],
        high_filt_th=config["core_tool_configs"]["polypolish"]["max_th"]
    envmodules:
        'singularity'
    shell:
        """
        singularity --silent run --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.bwa_sif} bwa index {input.assembly}
        singularity --silent run --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.bwa_sif} bwa mem -t 32 -a {input.assembly} {input.read_1} > {output.alignments_1}
        singularity --silent run --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.bwa_sif} bwa mem -t 32 -a {input.assembly} {input.read_2} > {output.alignments_2}
        singularity --silent run --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.sif_file} polypolish filter --in1 {output.alignments_1} --in2 {output.alignments_2} --out1 {output.filt_align_1} --out2 {output.filt_align_2}
        singularity --silent run --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.sif_file} polypolish polish {input.assembly} {output.filt_align_1} {output.filt_align_2} > {output.poly_polished_fasta}
        """


rule hybrid_polishing:
    input:
        sif_file = config["polca_sif"],
        poly_polished_fasta = config['output_directory']+'{sample_id_pattern}_poly_polished.fasta',
        reads_R1 = config['output_directory']+'{sample_id_pattern}_fastp_1.fastq.gz',
        reads_R2 = config['output_directory']+'{sample_id_pattern}_fastp_2.fastq.gz',
    output:
        config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.PolcaCorrected.fa',
        config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.report',
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.alignSorted.bam'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.alignSorted.bam.bai'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.batches'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.bwa.amb'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.bwa.ann'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.bwa.bwt'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.bwa.pac'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.bwa.sa'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.fai'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.fix.success'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.index.success'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.map.success'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.names'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.report.success'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.sort.success'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.unSorted.sam'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.vcf'),
        temp(config['output_directory']+'{sample_id_pattern}_poly_polished.fasta.vc.success')
    params:
        threads=config["core_tool_configs"]["polca"]["threads"],
        memory=config["core_tool_configs"]["polca"]["ram_gbp_thread"]
    envmodules:
        'singularity'
    shell:
        """
        cd {config[output_directory]}
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} \
        {input.sif_file} polca.sh -a {input.poly_polished_fasta} -r '{input.reads_R1} {input.reads_R2}' -t {params.threads} -m {params.memory}
        """


rule filter_host:
    input:
        kraken_sif = ancient(config["kraken2_sif"]),     
        pigz_sif = ancient(config["pigz_sif"]),
        read_1 = ancient(config['output_directory']+'{sample_id_pattern}_fastp_1.fastq.gz'), # quality-trimmed reads
        read_2 = ancient(config['output_directory']+'{sample_id_pattern}_fastp_2.fastq.gz')
    output:                                                                                                         
        temp(config['output_directory']+'{sample_id_pattern}_host_1.fastq'), # host reads (temp)
        temp(config['output_directory']+'{sample_id_pattern}_host_2.fastq'),                                        
        sample_1       = config['output_directory']+'{sample_id_pattern}_host_filtered_1.fastq.gz', # sample reads
        sample_2       = config['output_directory']+'{sample_id_pattern}_host_filtered_2.fastq.gz',
        report_name    = config['output_directory']+'{sample_id_pattern}_kraken2_host_filtering_report.txt',
        filtering_data = temp(config['output_directory']+'{sample_id_pattern}_kraken2_host_filtering_data.txt'),
        stdout         = temp(config['output_directory']+'{sample_id_pattern}_hf_k2_stdout') # kraken2 runtime report
    threads: 
        config["core_tool_configs"]["kraken2"]["threads"]
    resources:    
        mem_mb = config["core_tool_configs"]["kraken2"]["ram_host_mb"]
    envmodules:
        'singularity'
    # conda:
    #     config["kraken2_env"]
    shell:                                                                                              
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][human_db]}:{config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][human_db]} {input.kraken_sif} \
        kraken2 --threads {threads} \
        --db {config[core_tool_configs][kraken2][human_db]} \
        --classified-out {config[output_directory]}{wildcards.sample_id_pattern}_host#.fastq \
        --unclassified-out {config[output_directory]}{wildcards.sample_id_pattern}_host_filtered#.fastq \
        --report {output.report_name} --output {output.filtering_data} --gzip-compressed \
        --paired {input.read_1} {input.read_2} > {output.stdout}

        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_host_filtered_1.fastq
        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_host_filtered_2.fastq
        """


rule illumina_assembly:
    input:
        sif_file = ancient(config["shovill_sif"]),
        read_1   = ancient(config['output_directory']+'{sample_id_pattern}_host_filtered_1.fastq.gz'),
        read_2   = ancient(config['output_directory']+'{sample_id_pattern}_host_filtered_2.fastq.gz'),
    threads: 
        config["core_tool_configs"]["shovill"]["cpus"]
    output:
        config['output_directory']+'{sample_id_pattern}_contigs/contigs.fa'                                #removed after renaming 
    envmodules:
        'singularity'
    shell:
        """
        FOLDER_NAME=/scratch/{wildcards.sample_id_pattern}_assembly_tmp_$(date +%Y-%m-%d_%H-%M-%S)
        cleanup_scratch() {{
            rm -rf $FOLDER_NAME
        }}
        trap cleanup_scratch EXIT
        ulimit -n 2048
        mkdir -m 775 $FOLDER_NAME
        singularity --silent exec --bind ${{FOLDER_NAME}},{config[output_directory]},{config[work_dir]}:${{FOLDER_NAME}},{config[output_directory]},{config[work_dir]} {input.sif_file} shovill {config[core_tool_configs][shovill][modules]} \
        --depth {config[core_tool_configs][shovill][depth]} --ram {config[core_tool_configs][shovill][ram]} \
        --cpus {threads} --minlen {config[core_tool_configs][shovill][minlen]} \
        --mincov {config[core_tool_configs][shovill][mincov]} \
        --force --outdir {config[output_directory]}{wildcards.sample_id_pattern}_contigs \
        --R1 {input.read_1} --R2 {input.read_2}\
        --tmpdir $FOLDER_NAME

        rm -r $FOLDER_NAME
        """


rule contig_id:
    input: 
        choose_assembly_branch
    output:
        contigs = config['output_directory']+'{sample_id_pattern}_contigs.fasta'
    shell:
        """
        cp {input} {output}
        """


rule classify_reads:
    input:
        krona_sif = ancient(config["krona_sif"]),         #kreport2krona converts kraken2 output to krona format & ktImportText converts the result to interactive format
        kraken_sif = ancient(config["kraken2_sif"]),
        pigz_sif = ancient(config["pigz_sif"]),
        read_1   = ancient(config['output_directory']+'{sample_id_pattern}_host_filtered_1.fastq.gz'), # quality-trimmed, host-filtered reads
        read_2   = ancient(config['output_directory']+'{sample_id_pattern}_host_filtered_2.fastq.gz')    
    output:
        config['output_directory']+'{sample_id_pattern}_bact_reads_classified_1.fastq.gz', # classified reads
        config['output_directory']+'{sample_id_pattern}_bact_reads_classified_2.fastq.gz',
        config['output_directory']+'{sample_id_pattern}_bact_reads_unclassified_1.fastq.gz', # unclassified reads
        config['output_directory']+'{sample_id_pattern}_bact_reads_unclassified_2.fastq.gz',
        report_name = config['output_directory']+'{sample_id_pattern}_kraken2_reads_report.txt',
        filtering_data = temp(config['output_directory']+'{sample_id_pattern}_kraken2_read_data.txt')
    
    threads: config["core_tool_configs"]["kraken2"]["threads"]
    envmodules:
        'singularity'
    # conda:
    #     config["kraken2_env"]
    shell:                                                                                                          
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][bact_db]}:{config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][bact_db]} {input.kraken_sif} \
        kraken2 --threads {threads} --db {config[core_tool_configs][kraken2][bact_db]} \
        --classified-out {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_classified#.fastq \
        --unclassified-out {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_unclassified#.fastq \
        --report {output.report_name} --gzip-compressed --paired {input.read_1} {input.read_2} > {output.filtering_data}

        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.krona_sif} \
        kreport2krona.py -r {output.report_name} -o {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_reads_report.krona

        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.krona_sif} \
        ktImportText {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_reads_report.krona \
        -o {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_reads_report.html


        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_classified_1.fastq
        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_classified_2.fastq
        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_unclassified_1.fastq
        singularity --silent exec --bind {config[output_directory]}:{config[output_directory]} {input.pigz_sif} \
        pigz {config[output_directory]}{wildcards.sample_id_pattern}_bact_reads_unclassified_2.fastq
        """


rule classify_contigs:
    input:
        krona_sif = ancient(config["krona_sif"]),
        kraken_sif = ancient(config["kraken2_sif"]),
        pigz_sif = ancient(config["pigz_sif"]),
        contigs  = ancient(config['output_directory']+'{sample_id_pattern}_contigs.fasta')
    output:
        report_name    = config['output_directory']+'{sample_id_pattern}_kraken2_contigs_report.txt',
        filtering_data = temp(config['output_directory']+'{sample_id_pattern}_kraken2_contig_data.txt')
    threads: config["core_tool_configs"]["kraken2"]["threads"]
    envmodules:
        'singularity'
    # conda:
    #     config["kraken2_env"]
    shell:
        """
        singularity --silent exec --bind {config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][bact_db]}:{config[output_directory]},{config[work_dir]},{config[core_tool_configs][kraken2][bact_db]} {input.kraken_sif} \
        kraken2 --threads {threads} --db {config[core_tool_configs][kraken2][bact_db]} \
        --report {output.report_name} {input.contigs} > {output.filtering_data}

        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.krona_sif} \
        kreport2krona.py -r {output.report_name} -o {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_contigs_report.krona

        singularity --silent exec --bind {config[output_directory]},{config[work_dir]}:{config[output_directory]},{config[work_dir]} {input.krona_sif} \
        ktImportText {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_contigs_report.krona \
        -o {config[output_directory]}{wildcards.sample_id_pattern}_kraken2_contigs_report.html
        """
